s# Create the query corpus
## By popularity
First we wish to fill the query corpus with queries that represent popular queries. We can use historical popularity (e.g. Google Zeitgeist - which queries were most popular in the preceding year). We can also use immediate popularity (e.g. Google Trends, Bing Trends).

## Random
There is also value in filling the corpus with queries that have an element of randomness. This will allow us to see if any effects are dependent on popular topics. However, it is important to fill the random corpus with feasible queries (i.e. we don't want to make queries that have literally never been made before, or have only been made by 1 or 2 users).
I believe a reasonable choice will be to identify common "grammar templates" of search queries, and then populate the grammar templates with randomly generated nouns and verbs.

# Create the UGC result dataset
Collect a set of results by making queries from the query corpus across all the search engines. Then we will begin by performing manual classification that utilizes 
1) domain rules (e.g. wikipedia.org => classified as Wikipedia UGC, wiki. = > classified as other Wiki UGC)
2) Existing prediction software (e.g. Twitter humanizr, which identifies organizational Twitter accounts). We may wish to add one more classifying step to identify if the organizational Twitter account is a revenue-generating organization (McDonalds vs. City of Chicago)
3) A simple ML classifier to handle any other cases

Train, Validate, Test this classifier
Once classifier is "approved", we can move onto the "live stage".
We will perform live data analysis and collect it data for multiple weeks. Then using highlights from this dataset, we will write the "results" section of the paper. The live analysis will remain available, so if the results have changed by date of publication, readers can clearly view the algorithmic changes (either intentional or learned).

Differences between the broad study (regional?) versus a more targeted study (how search results develop over time).
For the targeted study, it is important that identical queries are made at very regular intervals. Performing this experiment with a very large corpus will likely strain our computational resources or cause rate limit concerns.

Therefore the targeted study will require a smaller, manually refined corpus.
